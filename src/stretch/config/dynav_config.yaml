# Encoder setup
# Encoder is used to compute per-object embeddings.
encoder: "siglip"
open_vocab_category_map_file: example_cat_map.json
tts_engine: "gTTS"

# Sparse Voxel Map parameters
voxel_size: 0.1
obs_min_height: 0.2  # Ignore things less than this high when planning motions
obs_max_height: 1.5  # Ignore things over this height (eg ceilings)
obs_min_density: 5  # This many points makes it an obstacle
exp_min_density: 1
min_points_per_voxel: 15  # Drop things below this density per voxel

# Padding
pad_obstacles: 2  # Add this many units (voxel_size) to the area around obstacles
min_pad_obstacles: 1  # Do not pad LESS than this amount, for safety.

local_radius: 0.5  # Area around the robot to mark as explored (kind of a hack)
add_local_every_step: False
remove_visited_from_obstacles: False
min_depth: 0.25
max_depth: 2.5

# Object detection parameters
detection:
  module: "detic"
  # module: "yolo"
  category_map_file: example_cat_map.json
  use_detic_viz: False

# Point cloud cleanup
filters:
  # Use a simple convolutional filter
  smooth_kernel_size: 3
  # smooth_kernel_size: 4
  # smooth_kernel_size: 0
  use_median_filter: True
  median_filter_size: 4
  # median_filter_size: 2
  median_filter_max_error: 0.01
  use_derivative_filter: True
  derivative_filter_threshold: 0.2
  # use_voxel_filter: True

# Exploration
agent:
  realtime:
    # This is the distance to pose graph nodes
    matching_distance: 0.5
    # This was 0.05 in Atharva's experiments
    # It is how close lidar spins have to be to be considered the same
    temporal_threshold: 0.1
    # Maximum number of observations to match with a pose graph node
    maximum_matched_observations: 10
    # Camera pose match threshold. Intuitively, there should already be a observation very similar to the current observation in the pose graph.
    camera_pose_match_threshold: 0.1
  use_realtime_updates: True
  realtime_rotation_steps: 4
  in_place_rotation_steps: 8  # If you are not moving the head, rotate more often
  sweep_head_on_update: False
  # in_place_rotation_steps: 4
  # sweep_head_on_update: True

# Instance memory parameters
# These are mostly around making sure that we reject views of objects that are too small, too spotty, too unreliable, etc.
instance_memory:
  min_instance_thickness: 0.01
  min_instance_vol: 1e-6
  max_instance_vol: 10.0
  min_instance_height: 0.01
  max_instance_height: 1.8
  min_pixels_for_instance_view: 100
  min_percent_for_instance_view: 0.1
  # Should we remove the background from the instance views?
  # What doe this mean? If you have a view of a bottle on a table, should we remove the table?
  # It will have an effect on performance.
  mask_cropped_instances: False  # Should we remove the background from the instance views?

# TAMP parameters
guarantee_instance_is_reachable: True
use_scene_graph: False
scene_graph:
  max_near_distance: 0.3
  min_on_height: 0.05
  max_on_height: 0.2

# Navigation space - used for motion planning and computing goals.
step_size: 0.1
rotation_step_size: 0.2 
dilate_frontier_size: 2  # Used to shrink the frontier back from the edges of the world
dilate_obstacle_size: 0  # Used when selecting navigation goals 
default_expand_frontier_size: 5  # margin along the frontier where final robot position can be
# Navigation space - used for motion planning and computing goals.
motion_planner:
  step_size: 0.05
  rotation_step_size: 0.1
  simplify_plans: True
  shortcut_plans: True
  shortcut_iter: 100
  # Parameters for frontier exploration using the motion planner.
  frontier:
    dilate_frontier_size: 2  # Used to shrink the frontier back from the edges of the world
    dilate_obstacle_size: 4  # Used when selecting goals and computing what the "frontier" is 
    default_expand_frontier_size: 12  # margin along the frontier where final robot position can be
    # Distance away you search for frontier points
    min_dist: 0.1
    # Subsampling frontier space at this discretization
    step_dist: 0.2
  goals:
    # manipulation_radius: 0.45
    manipulation_radius: 0.55

# Trajectory following - how closely we follow intermediate waypoints
# These should be less strict than whatever parameters the low-level controller is using; this will
# make sure that the motions end up looking smooth.
trajectory_pos_err_threshold: 0.15
trajectory_rot_err_threshold: 0.5
trajectory_per_step_timeout: 3.0

agent:
  realtime:
    # This is the distance to pose graph nodes
    matching_distance: 0.5
    # This was 0.05 in Atharva's exerimetns
    # It is how close lidar spins have to be to be considered the same
    temporal_threshold: 0.1
  realtime_rotation_steps: 4
  in_place_rotation_steps: 4
  sweep_head_on_update: True

# These are mostly around making sure that we reject views of objects that are too small, too spotty, too unreliable, etc.
instance_memory:
  min_instance_thickness: 0.01
  min_instance_vol: 1e-6
  max_instance_vol: 10.0
  min_instance_height: 0.01
  max_instance_height: 1.8
  min_pixels_for_instance_view: 100
  min_percent_for_instance_view: 0.1
  # Should we remove the background from the instance views?
  # What doe this mean? If you have a view of a bottle on a table, should we remove the table?
  # It will have an effect on performance.
  mask_cropped_instances: False  # Should we remove the background from the instance views?
  matching:
    # Feature matching threshold for if something is considered a particular class
    # Set this value by experimting with:
    #   python -m stretch.app.query --threshold 0.05
    # You can change the threshold to anything that makes sense.
    feature_match_threshold: 0.05
