# Encoder setup
# Encoder is used to compute per-object embeddings.
#encoder: "normalized_clip"
#encoder_args:
#  version: "ViT-B/32"
encoder: "siglip"
encoder_args:
  # Version options are ["base", "so400m"]
  version: "so400m"
  # Feature matching threshold for if something is considered a particular class
  # Set this value by experimting with:
  #   python -m stretch.app.query --threshold 0.05
  # You can change the threshold to anything that makes sense.
  feature_match_threshold: 0.1
  # version: "base"
  # feature_match_threshold: 0.05
open_vocab_category_map_file: example_cat_map.json
tts_engine: "gTTS"

voxel_size: 0.1
obs_min_height: 0.2  # Ignore things less than this high when planning motions
neg_obs_height: -0.05  # Things less than this height ARE obstacles
obs_max_height: 1.5  # Ignore things over this height (eg ceilings)
obs_min_density: 5  # This many points makes it an obstacle
exp_min_density: 1
min_points_per_voxel: 15  # Drop things below this density per voxel

# Padding
pad_obstacles: 2  # Add this many units (voxel_size) to the area around obstacles
min_pad_obstacles: 1  # Do not pad LESS than this amount, for safety.

local_radius: 0.8  # Area around the robot to mark as explored (kind of a hack)
add_local_every_step: False  # Add a local radius around the robot every step
remove_visited_from_obstacles: False
min_depth: 0.5
max_depth: 2.0

# Object detection parameters
# Recommendations for this part: 
# For yolo world, threshold should be 0.05 - 0.2, for Detic, threshold should be < 0.5
# If you want to detect as many objects in the environment, threshold should be low
# If you want to determine whether one object exists, 
# you'd better refer to the official documentation for the best threshold hyperparemeters
detection:
  module: "detic"  # Best performing method for our system
  # module: "mobile_sam"
  # module: "yolo"
  # module: "yolo_world"
  yolo_world_model_size: "l"   # Choose from ["s", "m", "l", "x"]
  yolo_confidence_threshold: 0.1
  confidence_threshold: 0.2
  category_map_file: example_cat_map.json  # This is used for Detic
  use_detic_viz: False

# Point cloud cleanup
# These parameters are used to clean up the point clouds going into the map we use for reasoning
# The idea is to remove noise and make the map more reliable. You can experiment with these values
# to see what works best for your environment.
filters:
  # Use a simple convolutional filter
  smooth_kernel_size: 3
  # smooth_kernel_size: 4
  # smooth_kernel_size: 0
  # Applies a median filter to the point cloud. Also drops outliers that are too far from the
  # median, to remove noise and streaking artifacts.
  use_median_filter: True
  median_filter_size: 4
  # median_filter_size: 2
  median_filter_max_error: 0.01
  # The derivative filter is supposed to drop points where the change in the point cloud is really
  # high to remove streaking artifacts. It is not currently used.
  use_derivative_filter: False
  derivative_filter_threshold: 0.1

# Motion convergence parameters
# These are to make sure the robot is not doing anything weird
motion:
  moving_threshold: 0.01  # How much the robot has to move to be considered "moving"
  angle_threshold: 0.1  # How much the robot has to rotate to be considered "rotating"
  min_steps_not_moving: 2  # How many steps the robot has to not move before we consider it "stopped"
  joint_tolerance:
    arm: 0.02
    base_x: 0.02
    lift: 0.02
    wrist_roll: 0.1
    wrist_pitch: 0.1
    wrist_yaw: 0.1
    head_pan: 0.01
    head_tilt: 0.01
  joint_thresholds:
    head_not_moving_tolerance: 1.0e-4
    gripper_open_threshold: 0.3

# Exploration
agent:
  realtime:
    # This is the distance to pose graph nodes
    matching_distance: 0.5
    # This was 0.05 in Atharva's experiments
    # It is how close lidar spins have to be to be considered the same
    temporal_threshold: 0.1
    # Maximum number of observations to match with a pose graph node
    maximum_matched_observations: 25
    # Camera pose match threshold. Intuitively, there should already be a observation very similar to the current observation in the pose graph.
    camera_pose_match_threshold: 0.05
  
  # Configuration of the realtime updates
  use_realtime_updates: True
  realtime_rotation_steps: 4

  # Configuration of the in-place rotation
  in_place_rotation_steps: 8  # If you are not moving the head, rotate more often
  sweep_head_on_update: False

# Instance memory parameters
# These are mostly around making sure that we reject views of objects that are too small, too spotty, too unreliable, etc.
instance_memory:
  min_instance_thickness: 0.01
  min_instance_vol: 1e-6
  max_instance_vol: 10.0
  min_instance_height: 0.01
  max_instance_height: 1.8
  min_pixels_for_instance_view: 100
  min_percent_for_instance_view: 0.1
  # Should we remove the background from the instance views?
  # What doe this mean? If you have a view of a bottle on a table, should we remove the table?
  # It will have an effect on performance.
  mask_cropped_instances: False  # Should we remove the background from the instance views?

# TAMP parameters
guarantee_instance_is_reachable: True
use_scene_graph: True
scene_graph:
  max_near_distance: 0.3
  min_on_height: 0.05
  max_on_height: 0.2

# Navigation space - used for motion planning and computing goals.
motion_planner:
  step_size: 0.05
  rotation_step_size: 0.1
  algorithm: "a_star"  # ["rrt", "rrt_connect", "a_star"]
  simplify_plans: False
  shortcut_plans: False
  simplify:
    max_step: 0.5
    min_step: 0.05
    num_steps: 8
    min_angle: 0.1
  shortcut_iter: 100
  # Parameters for frontier exploration using the motion planner.
  frontier:
    dilate_frontier_size: 2  # Used to shrink the frontier back from the edges of the world
    dilate_obstacle_size: 4  # Used when selecting goals and computing what the "frontier" is 
    default_expand_frontier_size: 12  # margin along the frontier where final robot position can be
    # Distance away you search for frontier points
    min_dist: 0.1
    # Subsampling frontier space at this discretization
    step_dist: 0.2
  goals:
    manipulation_radius: 0.45
    # manipulation_radius: 0.55

# Trajectory following - how closely we follow intermediate waypoints
# These should be less strict than whatever parameters the low-level controller is using; this will
# make sure that the motions end up looking smooth.
trajectory_pos_err_threshold: 0.15
trajectory_rot_err_threshold: 0.5
trajectory_per_step_timeout: 3.0
